%1234567890123456789012345678901234567890123456789012345678901234567890123456789
%         1         2         3         4         5         6         7        8

\documentclass[runningheads,a4paper]{llncs}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{url}

% \usepackage{breakurl}
\usepackage{float}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[tight]{subfigure}
\usepackage{wrapfig}

\usepackage{lipsum}
\newcommand{\BnL}[1][1em]{ \includegraphics[width=#1]{images/bnl.jpg} }
\newcommand{\teamori}{Team ORIon}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    anchorcolor=black,
    urlcolor=black,
    citecolor=black,
    filecolor=black
}

\begin{document}

\authorrunning{Nick Hawes et al.}

\title{\teamori\\ WRS 2018 Partner Robot Challenge\\(Real Space)}

\author{Nick Hawes \and Ioannis Havoutis \and Lars Kunze \and Bruno Lacerda \and Paul Amayo \and Julie Dequaire \and Rowan Border \and Stephen Kyberd}
\institute{Oxford Robotics Institute, University of Oxford, UK, \\
\texttt{nickh@robots.ox.ac.uk} \\
\url{http://ori.ox.ac.uk/robocup/}}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
This document outlines the approach \textit{\teamori{}} will take to the 
Partner Robot Challenge (Real Space) at the World Robot Summit 2018.
We are a new team that aspires to compete in international competitions,
starting from the 2018 World Robot Challenge event in Tokyo. Our research
interests are centred around long-term
autonomy, mobility, robot learning and knowledge representation. 
Advances in these directions will enable service robots to interact and
collaborate with humans,
and complete useful everyday tasks in typical household settings. 
We aim to demonstrate robust and intelligent autonomous behaviour, that uses
experience to learn and refine a growing set of robot skills, on the Toyota
Human Support Robot.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%The TDP is an 8-pages (+ annex) long scientific paper, detailing information on the technical and scientific approach of the team's research, while including also the following:
%\begin{itemize}
%	\item \checkmark Innovative technology and scientific contribution
%    \item \checkmark Focus of research/research interests
%    \item \checkmark Re-usability of the system for other research groups
%    \item \checkmark Applicability of the robot in the real world
%	\item \checkmark DSPL and SSPL: When the robot depicted in the TDP or Team Video is different from the league's standard one, the TDP must clearly state how the addressed approach and described software will be adapted to the standard platform robot.
%\end{itemize}

%Here are some references that can be relevant to our story:
%\cite{havoutis13ijrr,Winkler2015,havoutis15clawar,Mastalli2015,Havoutis16SSRR,Zeestraten2017-RAL,Havoutis17ICRA,Zeestraten17IROS,Mastalli17ICRA}.

\section{Introduction}

\textit{\teamori{}} is a new team created within the Oxford Robotics Institute
(ORI) at the University of Oxford. The team consists of undergraduate and
graduate students; robotics researchers; and faculty members of ORI. We come
from a strong research institute with seminal work in mobile autonomy and
machine learning. ORI has a significant track record in \emph{field robotics}
and real-world trials of autonomous systems. It also has a team of professional
hardware and software engineers. This experience and support is leveraged
to create a team capable of delivering across the whole
competition. 

The Partner Robot Challenge (Real Space) affords a tangible new domain in
which existing ORI research can be applied, and which provides new challenges
for the group. 
We have successfully applied for a Toyota Human Support Robot (HSR) and we are 
currently (\today) finalising the lease contract with Toyota.
The HSR allows us to focus on developing the
intelligence required for successfully completing the Partner Robot Challenge tasks, 
without the added burden of building and maintaining a custom platform. 
Competing in the WRS 2018 Partner Robot Challenge (Real Space) event in Tokyo will allow us to demonstrate 
the capabilities presented in this document and will provide valuable experience
for our future participation at international events.

\section{Team Composition}

The team is led by Prof. Nick Hawes, who has extensive background in
intelligent autonomous robots that can work with or for humans, and Dr. Ioannis Havoutis, an expert in combining motion planning with machine learning. The core of the team for 2018 will be ORI post-doctoral researchers (Lars Kunze, Bruno Lacerda) and senior PhD students (Paul Amayo, Julie Dequaire, Rowan Border), with more junior members (PhDs and undergraduates) being brought in as the team develops. 


\section{Robotics for Happiness}
The development of assistive and collaborative robots can have
a positive impact in the everyday life of people. Robots soon
will be able to alleviate us from many of the daily chores and routine tasks,
leaving much more time for people to spend in meaningful and enriching
activities. More free time would allow one, for example, to practice a hobby, 
to take walks through the park or to spend more time with family.
Such activities contribute positively to peoples' happiness and this is how
we envisage robotics contributing to happiness.

Through robotics for happiness we emphasize \emph{efficiency}, \emph{safety},
and \emph{collaboration} as key factors. Robots that are able perform their
tasks in an efficient and safe way in collaboration with people in everyday
environments are essential in many service-robot applications. Within our team
we have experience in deploying autonomous robots in real-world
environments including offices and care homes. During these deployments we have
experienced first-hand how robot technology can have a positive impact on
people's lives. Therefore we strongly believe that the focus of this year's 
World Robot Summit including the planned challenges are an excellent way to
promote and advance robot technology in this area.         

\section{Capabilities and Goals}

\begin{figure}[tb]
  \begin{center}
    \includegraphics[width=.43\columnwidth]{images/betty.jpg}
    \includegraphics[width=.55\columnwidth,clip,trim=10ex 20ex 10ex 20ex]{images/viewplanning_at_tsc.png}
  \end{center} 
  \vspace{-10pt}  
  \caption{\textit{Left}: The STRANDS autonomous mobile robot in a real-world
  office environment. \textit{Right}: View planning for object detection in the
  office environment.}
  \label{fig:mk}
  \vspace{-3ex}
\end{figure}

The capabilities of our robot system will build upon the capabilities developed over the last four years within the EU STRANDS Project\footnote{\url{http://strands-project.eu}}. Key members of this project (Nick Hawes, Lars Kunze, Bruno Lacerda) have recently moved to ORI and will be part of \teamori. The STRANDS Project deployed autonomous mobile robots (MetraLabs SCITOS A5, see Fig.~\ref{fig:mk}) in a range of human-populated environments for long durations~\cite{strands@ram}. These robots provided a range of services to real users, similar to the tasks required in the competition. The enabling software used on the robots, the ROS-based \emph{STRANDS Core System} (SCS), therefore gives \teamori{} an ideal basis for development. The SCS is open source, and \teamori{} will contribute to the continued maintenance of this substantial code base which is useful for our entire community. Although originally developed for MetraLabs robots, the SCS has recently been ported to other robots, and \teamori{} will contribute an open port of this software to the Toyota HSR. 


The SCS builds upon standard ROS components to provide the following capabilities, all of which have been tested in long-term deployments in real user environments: topological, human-aware robust navigation; object detection, identification and classification; autonomous online object learning; human detection, skeleton tracking, and activity analysis; basic human-robot interaction via speech and screen; and goal management and task planning. Our capabilities for person tracking~\cite{dondrup2015tracker} can be seen online\footnote{\url{https://youtu.be/zdnvhQU1YNo}} and formed the basis of many interactive behaviours, including social navigation, and activity learning and recognition~\cite{duckworth_aamas2016}, which are relevant to the Partner Robot Challenge. 

\subsection{Collaboration between humans and robots}

Collaboration between humans and robots, or human-robot interaction (HRI), is of major importance in many service robotics tasks. HRI is one of the truly challenging areas in autonomous robot systems research. In context of the STRANDS project~\cite{strands@ram}, our robots interacted with people in real-world environments in various ways: they guided visitors to places within a building, led a walking group of elderly people in a weekly therapy session, and provided information to people whereby they planned when and where to provide it most effectively. 
While our robots provided valuable assistance to people in several tasks, they also relied on human collaboration in a range of other situations. For example, as our  robots were not able to open doors they had to ask a person for help whenever they wanted to go to a different part of the environment. Similarly, whenever they got stuck and could not recover themselves (for various reasons) they asked people for help to get them out of the situation. In both situations, the HRI was realized via a combination of speech output and interaction on a touch screen. In the area of scene understanding, we investigated how robot can refer to objects in the environment using Natural Language~\cite{tufts17reg} to provide spatial descriptions to end-users.  

A fully competent HRI also involves also manipulation (e.g. hand-over), human-aware navigation, and semantic vision/scene understanding. These areas are discussed below.

\subsection{Manipulation -- Learning new skills}


In the context of manipulation, the robot will require a number of key skills 
to successfully perform a wide
variety of tasks that involve interaction with the environment or other
agents (Figure \ref{fig:baxter_water_task}), eg. pushing buttons, tuning handles, grasping and passing objects, etc. 
% 
The robots used in the STRANDS Project did not have manipulation capabilities, therefore the SCS does not provide software to support this. To deliver these capabilities we will start from \teamori{} member Lars Kunze's previous experience of knowledge-enabled manipulation~\cite{kunze15aij}. This previous work resulted in a system which could grasp an egg (\url{https://www.youtube.com/watch?v=jLz87H4q3hU}) and make a pancake (\url{https://www.youtube.com/watch?v=YQs5gRei8k4}). 
%
%Predicting and manually 
%designing in advance such a skill-set is only feasible for robots that perform 
%a narrow set of specific functions. 
To augment this we aim to build in our framework the ability
to learn and refine new skills as tasks change or as new tasks need to be added
the task repertoire. Such capability will be based on \teamori{} 
member Ioannis Havoutis' background in learning, synthesis and control of 
complex motions \cite{Havoutis16SSRR,Zeestraten18RAL}. Skill representations
are learnt from demonstrations---allowing also the use by non-experts---using a probabilistic generative encoding %, combining Gaussian 
%Mixture Models (GMMs) and Hidden semi-Markov Models (HSMMs)
\cite{Havoutis17ICRA}. Motion generation is formulated as an optimal
control problem that adapts to changing task configurations on-line \cite{Zeestraten17IROS,Zeestraten2017-RAL} (\url{https://youtu.be/NiRPE0egymk}).
\begin{figure*}[!t]
	\centering
	\subfigure{\resizebox{\textwidth}{!}{\includegraphics{images/baxter_learning_riemannian.png}}}
	\vspace{-10pt}%
	\caption{Snapshots of the Baxter robot performing a water pouring task that
	is learnt from demonstration \cite{Zeestraten2017-RAL}. The probabilistic
	encoding captures the correlation among task variables and produces a
	controller that generalizes the behaviour.}
	\label{fig:baxter_water_task}
	\vspace{-3ex}
\end{figure*}

\subsection{Navigation}

To enable robust navigation in all settings we take a hierarchical approach to navigation. The hierarchy is structured around a topological map in which discrete locations are connected by directed edges~\cite{jpulido2015NowOrLater}. Edges correspond to navigation actions the robot can perform to transition between locations. These may be standard \texttt{move\_base} actions, social navigation, closed loop controllers (such as wall following or door passing), or teach-and-repeat paths. Choices between the actions are made by a Markov decision process-based planner which jointly optimises for success probability and completion time, using probabilistic models learnt online through experience~\cite{LPH14b}. To ensure the robot does not get stuck we employ a monitored navigation layer which monitors the execution of the low level edge actions and performs recovery behaviours (e.g. backtracking, HRI) to correct observed problems~\cite{strands@ram}. This collection of techniques drove the STRANDS robots for over 360km of autonomous navigation in human-populated environments. For \teamori{} we will extend the framework to enable integration of ORI's visual teach-and-repeat paradigm, to enable the robot to navigate in areas where laser-based localisation is likely to result in imprecise navigation. We will also look to integrate some of ORI's previous 3D mapping work (e.g.~\cite{AmayoICRA2016}) to increase the accuracy of the robot's environment representation.

\subsection{Semantic Vision}

Learning and recognising objects during operation is a key task for a mobile service robot in human environments. \teamori{} will exploit the work done in the STRANDS Project in terms of autonomous object learning plus the recognition and modelling of previously unseen objects. The work is based on the \emph{meta-room} approach which builds dense RGB-D reconstructions of regions around locations in the robot's topological map. Objects are found through inspecting or differencing meta-rooms. Surfaces and possible objects found in meta-rooms become either targets for more detailed view planning~\cite{kunze14indirect} (see Figure~\ref{fig:mk}) leading to recognition, or for autonomous object learning~\cite{Faeulhammer:2016}. Our recognition pipeline mixes top-down semantic reasoning with bottom-up appearance-based processing for scene understanding \cite{kunze14topdown}, jointly estimating object locations and categories based on qualitative spatial models~\cite{kunze14bootstrapping}. The object learning process can build detailed 3D models entire without supervision~\cite{Faeulhammer:2016}. Previously unknown  objects are processed with a mix of deep vision and semantic web technologies to provide the robot with an initial estimate of their identity~\cite{aloof@icra17}.



\subsection{Robot System Integration}

A substantial effort will be needed to develop behaviours that are robust to
changes in the environment and to noise typical of real-world scenarios. In this
respect we will exploit our experience from the STANDS project 
\cite{strands@ram} and build upon the tested SCS. 
Given the similarity of the HSR to the SCITOS platform, and other platforms the team are familiar with, %(e.g. through Lars Kunze's work on the PR2 for object search in a multi-story building~\cite{kunze12objsearch}\footnote{\url{https://www.youtube.com/watch?v=RIYRQC2iBp0}}), 
we predict that porting the
SCS to the Toyota HSR will be an easy task. SCS was developed to be 
expandable and is built with standard ROS components which are also supported by the
Toyota HSR. Additionally, we are planning to build a mock-up of the ``house''
arena to
allow us to run live robot trials and limit simulation use to the development
phase. We aim to schedule recurring trials as our framework is developed, to
ensure that robot behaviours are successful and to collect data on the 
success probabilities of tasks and sequences of tasks.

\teamori{} benefits from the many years of experience of the team of creating integrated robot systems. Team members (including Julie Dequaire) contributed to the first public demonstration of a self-driving car in the UK\footnote{\url{https://www.epsrc.ac.uk/newsevents/news/lutzpathfinder/}}, and all members have contributed to integrated robot systems demonstrated at science museums, public engagement events and trade shows across Europe. All of these systems integrate perception, planning and action in non-trivial ways. Such integration is central to producing a functional and reliable system, but can be incredibly challenging when trying to produce novel capabilities for robots in task environments which you are only able to experience a short time before a deadline. The team's joint experience of bringing diverse robot capabilities together for successful demos will enable the team to start working effectively very quickly, and to deal with common team and system teething problems smoothly. Our experience on systems which span the capability spectrum from low-level sensing to high-level cognition means that the diverse capabilities described above will be successfully integrated to produce a competitive entry in the 2018 Partner Robot Challenge event in Tokyo.


\section{Applicability and Re-Usability of the System}


%     \item Re-usability of the system for other research groups
%     \item Applicability of the approach in the real world


The continued maintenance and development of the SCS will provide a well-tested
software framework for mobile service robots. Continuing the practice started by
the STRANDS Project, we will make extensions to the SCS available as open source
software. This will enable our core framework to be reusable by other groups.
We expect this to lead to a spillover effect and be the starting point for
long-lasting collaborations with research partners interested in
assistive and collaborative robotics.
The validity of this approach has already been demonstrated by the reuse of the
SCS at labs including the Intelligent Robots and Systems group at the Institute
for Systems and Robotics, Lisbon, and at Honda Research Institute Europe. The
aforementioned use of the majority of our technology within systems which have
already been successfully demonstrated in real challenging service robot
environments shows that our approach is applicable in the real world.

\section{Conclusion}
\textit{\teamori{}} is a new team that builds on the strong research 
background of its team members and on extensive real-world robot operating
experience, in a range of service tasks, similar to the Partner Robot Challenge. 

Competing in the 2018 Partner Robot Challenge (Real Space) event in Tokyo will allow us to demonstrate 
the presented capabilities on the HSR and will provide valuable experience
for our newly-born team.

\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document} 
