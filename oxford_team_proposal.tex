%1234567890123456789012345678901234567890123456789012345678901234567890123456789
%         1         2         3         4         5         6         7        8

\documentclass[runningheads,a4paper]{llncs}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{url}

% \usepackage{breakurl}
\usepackage{float}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[tight]{subfigure}
\usepackage{wrapfig}

\usepackage{lipsum}
\newcommand{\BnL}[1][1em]{ \includegraphics[width=#1]{images/bnl.jpg} }
\newcommand{\teamori}{Team ORIon}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    anchorcolor=black,
    urlcolor=black,
    citecolor=black,
    filecolor=black
}
%https://www.youtube.com/watch?v=sZ_oqFDUsnM
\begin{document}

\authorrunning{Ioannis Havoutis et al.}

\title{\teamori{} --- WRS 2020 Team Description}

\author{Ioannis Havoutis \and Nick Hawes \and Lars Kunze \and Bruno Lacerda 
\and Mark Finean \and Charlie Street \and Marc Rigter \and Chia-Man Hung}
\institute{Oxford Robotics Institute, University of Oxford, UK, \\
\texttt{mfinean@robots.ox.ac.uk} \\
%\url{https://ori.ox.ac.uk/projects/robocup/}
}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
This document outlines the approach \textit{\teamori} will take to the 
World Robot Summit (WRS) 2020 Partner (Real Space) competition. 
With a strong research background, we aspire to compete 
in this year's WRS competition. Our first experience competing with the Toyota Human 
Support Robot (HSR) was at WRS 2018, where we competed in the Partner 
Robot Challenge. We have since competed at RoboCup 2019 in the DSPL@Home league.
Our research interests are centred around long-term
autonomy, mobility, robot learning and knowledge representation. We aim to demonstrate robust and intelligent autonomous behaviour, that uses
experience to learn and refine a growing set of robot skills, on the Toyota
HSR.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%The TDP is an 8-pages (+ annex) long scientific paper, detailing information on the technical and scientific approach of the team's research, while including also the following:
%\begin{itemize}
%	\item \checkmark Innovative technology and scientific contribution
%    \item \checkmark Focus of research/research interests
%    \item \checkmark Re-usability of the system for other research groups
%    \item \checkmark Applicability of the robot in the real world
%	\item \checkmark DSPL and SSPL: When the robot depicted in the TDP or Team Video is different from the league's standard one, the TDP must clearly state how the addressed approach and described software will be adapted to the standard platform robot.
%\end{itemize}

%Here are some references that can be relevant to our story:
%\cite{havoutis13ijrr,Winkler2015,havoutis15clawar,Mastalli2015,Havoutis16SSRR,Zeestraten2017-RAL,Havoutis17ICRA,Zeestraten17IROS,Mastalli17ICRA}.

\section{Introduction}

\textit{\teamori{}} is a relatively new team created within the Oxford Robotics Institute
(ORI) at the University of Oxford. The team is led by Prof. Nick Hawes, who has extensive background in intelligent autonomous robots that can work with or for humans, and Dr. Ioannis Havoutis, an expert in combining motion planning with machine learning. The core of the team for 2020 will be ORI post-doctoral researchers and PhD students, and undergraduate members from the Department of Engineering Science of the University of Oxford. We come from a strong research institute with seminal work in mobile autonomy and machine learning. ORI has a significant track record in \emph{field robotics} and real-world trials of autonomous systems. It also has a team of professional
hardware and software engineers. This experience and support is leveraged
to create a robotics competition team capable of delivering across the whole
competition. 

The WRS Partner (Real Space) league affords a tangible domain in
which existing ORI research can be applied, and which provides new challenges
for the group. We competed in the Partner Robot Challenge of WRS in the October 2018 and have since competed in the 2019 RoboCup@Home competition, placing $6^{th}$ out of 10 on our first attempt (see Figure~\ref{fig:robocup}). 
The HSR robot allows us to focus on developing the
intelligence required for successfully completing the WRS tasks, 
without the added burden of building and maintaining a custom platform. 
Competing in the 2020 WRS event will allow us to demonstrate 
the capabilities presented in this document and will provide valuable experience
for our future participations.

\begin{figure}[tb]
  \begin{center}
    \includegraphics[width=.48\columnwidth, clip, trim=0 0ex 0ex 50ex]{images/robocup_team.jpg}
    \includegraphics[width=.45\columnwidth, clip, trim=0 0ex 0ex 50ex]{images/robocup_team_working.jpg}
  \end{center} 
  \caption{\emph{Left:} Team ORIon at RoboCup@Home (WRS) 2019 in Sydney, Australia. \emph{Right:} The team hard at work developing for a specific task.}
  \label{fig:robocup}
\end{figure}

\section{Technical Challenge}
\subsection{Technical Challenges for 4S}
\subsubsection{Speed}
There is a careful balance in determining the speed at which a robot should operate. For accomplishing tasks around the home, we typically want them performed as fast as possible, while also minimising the risk of breakages and harm to either humans or the robot. To address this, the speed of base movement is adjusted based on the distance to obstacles. In clear areas of space, the robot is allowed to move much faster. Similarly, when then robot arm is operating in a more confined space, the speed of movements is restricted.

\subsubsection{Smooth/Smart}
The impression of a smooth and intelligent robot can also be derived from having a robust robot that uses feedback to verify its actions. In RoboCup@Home 2019, we used force-torque calculations on the wrist to inform the robot whether objects had been released from the gripper. When trying to manipulate a small object our HSR will first try to segment point clouds to generate an optimal grasp pose. In the case of failure, it will resort to a visual feedback system in the hand camera to attempt the task. For key phrases, we implement hotword detection to reduce execution times and to give communication with the robot a more natural and fluid feel. We also provide smart, robust behaviours at the task level by using state machines as an execution framework. This allows the robot to continue acting in the environment even after something unexpected has happened and provides much smarter behaviours than a hard coded approach.

\subsubsection{Stable}
We are interested in robots with robust behaviours that can react to failure at the point of execution. Our approach is to explicitly model the uncertainty in the environment at planning time. By doing this, rather than computing fixed plans over our robots behaviour, we can compute \emph{policies}. If something unexpected occurs at execution-time, our robots are able to react to this dynamically. For example, if a door we expect to be open is actually closed, we can take an alternate route, or ask for help \cite{lacerda2019probabilistic}.

\subsubsection{Safe}
Safety is our priority in deploying real robot systems. We firstly must avoid conducting any dangerous behaviour towards humans and the environment, and also to the robot. With the HSR, primary areas of concern for safety are during the navigation and manipulation stages of a task. We implement obstacle inflation in order to provide forbidden zones for the robot, preventing damage to surroundings and itself. Code provided by the STRANDS project provides our HSR with safe behaviours. We carry out high level navigation planning on a \emph{topological map} of the environment and whenever the robot traverses an edge on this map, data is collected about the success and duration of the navigation. Our robots collect data over long periods of time, which feeds back into our navigation planning. We model the uncertainty of success in the environment which allows us to plan to obtain \emph{probabilistic formal guarantees}, providing us with a guaranteed level of success and safety. Our monitored navigation framework provides a hierarchy of recovery behaviours, such as requesting help from nearby humans (detailed further in Section \ref{nav}).

\subsection{System Design}

\subsubsection{Keep Moving - }
The STRANDS' person detection continues to operate and enable robot locomotion, albeit at lesser speeds when in the presence of humans.

\subsubsection{Move carefully - }
The 2D laser scanner enables the robot to maintain safe distances from it's surroundings such as walls. In the first WRS competition we attended, we noticed that the HSR would sometimes reverse without checking if their are objects behind it first. We have since rectified this in order to maxmimise safety. The STRANDS project 3D obstacle avoidance uses not only a planar 2D laser scanner for obstacle detections but also a downward-looking chest RGBD cam. We aim to incorporate this technology on the HSR platform by utilising the HSR's RGBD head camera while moving. 

\subsubsection{Be Clever - }
Our monitored navigation package detects navigation errors and enables the robot to recover automatically, for example by backtracking when it becomes uncertain or stuck. 

\section{Software development policy}
\subsection{Tools}
Git is used for version control. Communication within the team is typically executed via Slack.

\subsection{Open source software used by your team.}
GPD and GPG packages are used to generate and rank grasp poses on a segmented point cloud \cite{GPD1} \cite{GPD2}. We use `MoveIt!' in conjunction with the Toyota motion planner implementations to provide motion planning solutions \cite{MoveIt1} \cite{MoveIt2}. Point Cloud Library (PCL) is used to perform point cloud segmentations, primarily to feed into our grasp synthesis/manipulation pipeline \cite{PCL}. We use YOLOv2 to perform our object detection \cite{yolo}. We use and intend to incorporate many elements from the EU STRANDS Project\footnote{\url{http://strands-project.eu}} further detailed in the `Additional Innovative Technologies' section. \cite{strands@ram} \cite{dondrup2015tracker} \cite{duckworth_aamas2016}. Google Speech-to-Text API is our primary method to transcribe speech.  PocketSphinx and WaveNet provide alternative speech transcription in case of network connection failure \cite{pocketsphinx} \cite{wavenet}.

\subsection{Role assignment policy of each member}
New team members are encouraged to pursue the area of research which interests them most and contribute to the respective team. Sub-teams determine their sub-group leader of that area of research - typically a post-graduate student since they are most involved in the research. Tasks are formulated by sub-team leaders and presented  to the sub-team for members to decide which tasks best suit their skills set. Sub-teams are coordinated by the Team Leader who keeps track of the progress of sub-teams and coordinates integration, testing and any group sessions that might be required.

\section{Additional Innovative technologies}
\begin{figure}[tb]
  \begin{center}
    \includegraphics[width=.43\columnwidth]{images/betty.jpg}
    \includegraphics[width=.55\columnwidth,clip,trim=10ex 20ex 10ex 20ex]{images/viewplanning_at_tsc.png}
  \end{center} 
  \vspace{-10pt}  
  \caption{\textit{Left}: The STRANDS autonomous mobile robot in a real-world
  office environment. \textit{Right}: View planning for object detection in the
  office environment.}
  \label{fig:mk}
  \vspace{-3ex}
\end{figure}
The capabilities of our system build upon the EU STRANDS Project. Key members of this project (Nick Hawes, Lars Kunze, Bruno Lacerda) are part of \teamori. The STRANDS Project deployed autonomous mobile robots (MetraLabs SCITOS A5, see Fig.~\ref{fig:mk}) in a range of human-populated environments for long durations~\cite{strands@ram}. These robots provided a range of services to real users, similar to the tasks required in the WRS. The enabling software used on the robots, the ROS-based \emph{STRANDS Core System} (SCS), therefore gives \teamori{} an ideal basis for development towards tasks in the Partner (Real Space) league. The SCS builds upon standard ROS components to provide the following capabilities, all of which have been tested in long-term deployments in real user environments: topological, human-aware robust navigation; object detection, identification and classification; autonomous online object learning; human detection, skeleton tracking, and activity analysis; basic human-robot interaction via speech and screen; and goal management and task planning. Our capabilities for person tracking~\cite{dondrup2015tracker} can be seen online\footnote{\url{https://youtu.be/zdnvhQU1YNo}} and formed the basis of many interactive behaviours, including social navigation, and activity learning and recognition~\cite{duckworth_aamas2016}, which are relevant to the WRS tasks. 


\subsection{Manipulation -- Learning new skills}

The robot will require a number of key skills to successfully perform a wide
variety of tasks that involve interaction with the environment or other
agents (Figure \ref{fig:baxter_water_task}). To learn and refine new skills as tasks change, or as new tasks need to be added the task repertoire, capabilities will be based on \teamori{} member Ioannis Havoutis' background in learning, synthesis and control of 
complex motions \cite{Havoutis16SSRR}. Skill representations
are learnt from demonstrations---allowing also the use by non-experts---using a probabilistic generative encoding \cite{Havoutis17ICRA}. Motion generation is formulated as an optimal control problem that adapts to changing task configurations on-line \cite{Zeestraten17IROS,Zeestraten2017-RAL} (\url{https://youtu.be/NiRPE0egymk}).
\begin{figure*}[!t]
	\centering
	\subfigure{\resizebox{\textwidth}{!}{\includegraphics{images/baxter_learning_riemannian.png}}}
	\vspace{-10pt}%
	\caption{Snapshots of the Baxter robot performing a water pouring task that
	is learnt from demonstration \cite{Zeestraten2017-RAL}. The probabilistic
	encoding captures the correlation among task variables and produces a
	controller that generalizes the behaviour.}
	\label{fig:baxter_water_task}
	\vspace{-3ex}
\end{figure*}
Team Leader, Mark Finean, has adapted the manipulation stack to incorporate grasp synthesis on the object point clouds. The object recognition system identifies the pose of the object which is used for segmenting the point cloud. This is fed to our grasp synthesis implementation of GPD \cite{GPD1} \cite{GPD2}. Grasp synthesis outputs the 6-dimensional pose of approach for the highest ranked grasp. The approach described as above is less reliable for small objects such as cutlery. Instead, we implement a custom visual feedback system that uses the in-hand camera to align and orient the end effector to ensure a successful grasp.  

\subsection{Navigation}\label{nav}

To enable robust navigation, we take a hierarchical approach to navigation. The hierarchy is structured around a topological map in which discrete locations are connected by directed edges~\cite{jpulido2015NowOrLater}. Edges correspond to navigation actions the robot can perform to transition between locations. These may be standard \texttt{move\_base} actions, social navigation, closed loop controllers (such as wall following or door passing), or teach-and-repeat paths. Choices between the actions are made by a Markov decision process-based planner which jointly optimises for success probability and completion time, using probabilistic models learnt online through experience~\cite{LPH14b}. To ensure the robot does not get stuck we employ a monitored navigation layer which monitors the execution of the low level edge actions and performs recovery behaviours (e.g. backtracking, HRI) to correct observed problems~\cite{strands@ram}. We will extend the framework to enable integration of ORI's visual teach-and-repeat paradigm, to enable the robot to navigate in areas where laser-based localisation is likely to result in imprecise navigation. 

\subsection{Semantic Perception \& Mapping}
\teamori{} will exploit the work done in the STRANDS Project in terms of autonomous object learning plus the recognition and modelling of previously unseen objects. The work is based on the \emph{meta-room} approach which builds dense RGB-D reconstructions of regions around locations in the robot's Semantic Object Map (SOMA, \cite{SOMA18_kunze}). Objects are found through inspecting or differencing meta-rooms. Surfaces and possible objects found in meta-rooms become either targets for more detailed view planning~\cite{kunze14indirect,kunze17ecmr} (see Figure~\ref{fig:mk}) leading to recognition, or for autonomous object learning~\cite{Faeulhammer:2016}. Our recognition pipeline mixes top-down semantic reasoning with bottom-up appearance-based processing for scene understanding \cite{kunze14topdown}, jointly estimating object locations and categories based on qualitative spatial models\cite{kunze14bootstrapping}. The object learning process can build detailed 3D models entire without supervision~\cite{Faeulhammer:2016}. Previously unknown objects are processed with a mix of deep vision and semantic web technologies to provide the robot with an initial estimate of their identity~\cite{aloof@icra17}.

\section{Relevant publications }
Relevant publications are mentioned in the body of this document and provided in the references section.

\section{Energy efficiency and conservation efforts}
Within the STRANDS project, we implemented smart battery management.This was aimed towards an efficient handling of tasks and maximising long-term autonomy.

\section{Link to Team Video, Team Website}
Team Website: \url{https://ori.ox.ac.uk/projects/robocup/} \\
Team Video: \url{https://youtu.be/8QhZzqdJImU} 


\section{Conclusion}
\textit{\teamori{}} is approximately a year-old Toyota HSR team that builds on the strong research 
background of its team members and on extensive real-world robot operating
experience, in a range of service tasks, similar to the WRS Partner (Real Space) tasks. Competing in the 2020 WRS event in Japan will allow us to demonstrate 
the presented capabilities on the HSR and will provide valuable experience
for our up-and-coming team.

\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document} 