%1234567890123456789012345678901234567890123456789012345678901234567890123456789
%         1         2         3         4         5         6         7        8

\documentclass[runningheads,a4paper]{llncs}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{url}

% \usepackage{breakurl}
\usepackage{float}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[tight]{subfigure}
\usepackage{wrapfig}

\usepackage{lipsum}
\newcommand{\BnL}[1][1em]{ \includegraphics[width=#1]{images/bnl.jpg} }
\newcommand{\teamori}{Team Oxford}

\begin{document}

\title{Team ORIon\\ 2018 DSPL Team Description}

\author{Nick Hawes \and Ioannis Havoutis \and Lars Kunze \and Bruno Lacerda \and Paul Amayo \and Julie Dequaire }
\institute{Oxford Robotics Institute, University of Oxford, UK, \\
\texttt{"}}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
This document outlines the approach \textit{\teamori{}} will take to the 
RoboCup@Home Domestic Standard Platform League. 
We are a new team that aspires to compete in international competitions,
starting from 2018. Our research interests are centered around long-term
autonomy, mobility, robot learning and knowledge representation. 
Advances in these directions will enable service robots to interact with humans
and complete useful everyday tasks in typical household settings. 
We aim to demonstrate robust and intelligent autonomous behaviour, that uses
experience to learn and refine a growing set of robot skills, on the Toyota
Human Support Robot.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% It must contain the following information:
% \begin{itemize}
%     \item Description of the approach planned to be implemented on the robot
%     \item List of externally available components that are planned to be
%     implemented (Open source software, web services, etc.)
%     \item Innovative technology and scientific contribution
%     \item Focus of research/research interests
%     \item Re-usability of the system for other research groups
%     \item Applicability of the approach in the real world
% \end{itemize}
% The Proposal should go into detail about the technical and scientific approach


%Here are some references that can be relevant to our story:
%\cite{havoutis13ijrr,Winkler2015,havoutis15clawar,Mastalli2015,Havoutis16SSRR,Zeestraten2017-RAL,Havoutis17ICRA,Zeestraten17IROS,Mastalli17ICRA}.

\section{Introduction}
\textit{\teamori{}} is a new team created within the Oxford Robotics Institute
(ORI) at the University of Oxford. The team consists of undergraduate and
graduate students; robotics researchers; and faculty members of ORI. We come
from a strong research institute with seminal work in mobile autonomy and
machine learning. ORI has a significant track record in \emph{field robotics}
and real-world trials of autonomous systems. It also has a team of professional
hardware and software engineers. This experience and support will be leveraged
to create a RoboCup@Home team capable of delivering across the whole
competition. 

The Toyota Human Support Robot (HSR) will allow us to focus on developing the
intelligence required for successfully completing the Robocup@Home tasks, 
without the added burden of building and maintaining a custom platform.
The Domestic Standard Platform League (DSPL) affords a tangible new domain in
which existing ORI research can be applied, and which provides new challenges
for the group. 

\section{Team Composition}

The team is led by Prof. Nick Hawes, who has extensive background in
intelligent autonomous robots that can work with or for humans, and Dr. Ioannis Havoutis, an expert in combining motion planning with machine learning. The core of the team for 2018 will be ORI post-doctoral researchers (Lars Kunze, Bruno Lacerda) and senior PhD students (Paul Amayo, Julie Dequaire), with more junior members (PhDs and undergraduates) being brought in as the team develops. 

\section{Capabilities and Goals}

\begin{figure}[tb]
  \begin{center}
    \includegraphics[width=.43\columnwidth]{images/betty.jpg}
    \includegraphics[width=.55\columnwidth,clip,trim=10ex 20ex 10ex 20ex]{images/viewplanning_at_tsc.png}
  \end{center}   
  \caption{\textit{Left}: The STRANDS autonomous mobile robot in a real-world
  office environment. \textit{Right}: View planning for object detection in the
  office environment.}
  \label{fig:mk}
  \vspace{-3ex}
\end{figure}

The capabilities of our DSPL system will build upon the capabilities developed over the last four years within the EU STRANDS Project\footnote{\url{http://strands-project.eu}}. Key members of this project (Nick Hawes, Lars Kunze, Bruno Lacerda) have recently moved to ORI will actively engage with \teamori. The STRANDS Project deployed autonomous mobile robots (MetraLabs SCITOS A5s) in a range of human-populated environments for long durations\cite{strands@ram}. These robots provided a range of services to real users similar to the tasks required in the DSPL, and therefore the enabling software used on the robots (the ROS-based \emph{STRANDS Core System} (SCS)) gives \teamori{} the ideal basis for DSPL participation. The SCS is open source, and creating a team for the DSPL will contribute to the continued maintainence of this substantial code base which is useful for our entire community. Although originally developed for MetraLabs robots, the SCS has recently been ported to other robots, and \teamori{} will contribute an open port of this software to the Toyota HSR. 

The SCS builds upon standard ROS components to provide the following capabilities, all of which have been tested in long-term deployments in real user environments: topological, human-aware robust navigation; object dectection, identification and classification; autonomous online object learning; human detection, skeleton tracking, and activity analysis; basic human-robot interaction via speech and screen; and goal management and task planning. Our capabilities for person tracking~\cite{dondrup2015tracker} can be seen online\footnote{\url{https://youtu.be/zdnvhQU1YNo}} and formed the basis of many interactive behaviours, including social navigation, and activity learning and recognition~\cite{duckworth_aamas2016}, which are relevant to DSPL. Advanced object perception will be provided by our approach which combines view planning (see Figure~\ref{fig:mk}), top-down reasoning and bottom-up processing for scene understanding \cite{kunze14topdown}, predicts object locations based on qualitative spatial models\cite{kunze14bootstrapping}, and performs object discovery and classification via deep vision and web-mining \cite{aloof@icra17}.

The robots used in the STRANDS Project did not have manipulation capabilities, therefore the SCS does not provide software to support this. To deliver these capabilities we will start from \teamori{} member Lars Kunze's previous experience of knowledge-enabled manipulation~\cite{kunze15aij}. This previous work resulted in a system which could grasp an egg (\url{https://www.youtube.com/watch?v=jLz87H4q3hU}) and make a pancake (\url{https://www.youtube.com/watch?v=YQs5gRei8k4}).


\subsection{Manipulation -- Learning new skills}
In the context of manipulation, the robot will require a number of key skills 
to successfully perform a wide
variety of tasks that involve interaction with the environment or other
agents (Figure \ref{fig:baxter_water_task}), eg. pushing buttons, tuning handles, grasping and passing objects, etc. %Predicting and manually 
%designing in advance such a skill-set is only feasible for robots that perform 
%a narrow set of specific functions. 
We aim to build in our framework the ability
to learn and refine new skills as tasks change or as new tasks need to be added
the task repertoire. Such capability will be based on \teamori{} 
member Ioannis Havoutis' background in learning, synthesis and control of 
complex motions \cite{havoutis13ijrr,Havoutis16SSRR}. Skill representations
are learnt from demonstrations---allowing also the use by non-experts---using a probabilistic generative encoding, combining Gaussian 
Mixture Models (GMMs) and Hidden semi-Markov Models (HSMMs)
\cite{Havoutis17ICRA}. Motion generation is formulated as an optimal control
problem that can adapt to changing task parametrizations on-line \cite{Zeestraten17IROS,Zeestraten2017-RAL} (\url{https://youtu.be/NiRPE0egymk}).

\begin{figure*}[!tbh]
	\centering
	\subfigure{\resizebox{\textwidth}{!}{\includegraphics{images/baxter_learning_riemannian.png}}}
%	\vspace{-10pt}%
	\caption{Snapshots of the Baxter robot performing a water pouring task that
	is learnt from demonstration \cite{Zeestraten2017-RAL}. The probabilistic
	encoding captures the correlation between the two task frames and results
	in an optimal controller that reproduces and generalizes the behaviour.}
	\label{fig:baxter_water_task}
\end{figure*}

\subsection{Navigation(?)}
...


\paragraph{Capabilities}
\begin{itemize}

    \item Manipulation
    \item Integrated Robot Systems
        \begin{itemize}
            \item STRANDS project: \cite{strands@ram}
            \item Object search in real-world offices \cite{kunze14indirect}
            \item Object search in a multi-story building: \cite{kunze12objsearch} (Video: \url{https://www.youtube.com/watch?v=RIYRQC2iBp0})

        \end{itemize}
\end{itemize}


\section{Conclusion}

\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document} 
