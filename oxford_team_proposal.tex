%1234567890123456789012345678901234567890123456789012345678901234567890123456789
%         1         2         3         4         5         6         7        8

\documentclass[runningheads,a4paper]{llncs}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{float}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[tight]{subfigure}
\usepackage{wrapfig}

\usepackage{lipsum}
\newcommand{\BnL}[1][1em]{ \includegraphics[width=#1]{images/bnl.jpg} }

\begin{document}

\title{Lipsum 2017 Team Description Paper}

\author{Main-author \and Co-author \and Team Members }
\institute{Affiliation name and address, \\
\texttt{http://devoted-web-site.url}}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
This document outlines the organization, architecture and goals of our
RoboCup@Home Domestic Standard Platform League team, \textit{Team Oxford}(?). 
We are a new team that aspires to compete in international competitions,
starting from 2018. Our research interests are centered around long term
autonomy, mobility, robot learning and knowledge representation. 
Advances in these directions will enable service robots to interact with humans
and complete useful everyday tasks in typical household settings. 
We aim to demonstrate robust and intelligent autonomous behaviour, that uses
experience to learn and refine a growing set of robots skills, on the Toyota
Human Support Robot.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

It must contain the following information:
\begin{itemize}
    \item Description of the approach planned to be implemented on the robot
    \item List of externally available components that are planned to be
    implemented (Open source software, web services, etc.)
    \item Innovative technology and scientific contribution
    \item Focus of research/research interests
    \item Re-usability of the system for other research groups
    \item Applicability of the approach in the real world
\end{itemize}
The Proposal should go into detail about the technical and scientific approach


%Here are some references that can be relevant to our story:
%\cite{havoutis13ijrr,Winkler2015,havoutis15clawar,Mastalli2015,Havoutis16SSRR,Zeestraten2017-RAL,Havoutis17ICRA,Zeestraten17IROS,Mastalli17ICRA}.

\section{Introduction}
\textit{Team Oxford} is a new team created within the Oxford Robotics Institute
(ORI) at the University of Oxford. The team consists of graduate students,
robotics researchers and faculty members of ORI. We come from a strong research
institute with seminal work in mobile autonomy and machine learning.
The team is lead by Prof. Nick Hawes, who has extensive background in
intelligent autonomous robots that can work with or for humans.

The Domestic Standard Platform League affords a very tangible and 
interesting/challenging domain for our research work. In addition, the
Toyota Human Support Robot (HSR) will allow us to focus on developing the
intelligence required for successfully completing the Robocup@Home tasks, 
without the added burden of building and maintaining a custom platform.


\section{Team Composition and Research Interests}

\section{Capabilities and Goals}

\begin{figure*}[!ht]
	\centering
	\subfigure{\resizebox{\textwidth}{!}{\includegraphics{images/baxter_learning_riemannian.png}}\label{fig:
all:a}}	
%	\vspace{-10pt}%
	\caption{Learning a skill from demonstration using Riemannian manifold representation. Covariance/correlation to optimal controller.}
	\label{fig:baxter_water_task}
\end{figure*}

\paragraph{Capabilities}
\begin{itemize}
    \item Knowledge-enabled Perception
        \begin{itemize}
            \item Combined top-down reasoning and bottom-up perception for scene understanding \cite{kunze14topdown}, predicting object locations based on qualitative spatial models\cite{kunze14bootstrapping}, object discovery via deep vision and web-mining \cite{aloof@icra17}, joint object classification using situated robot perception and web-mining \cite{aloof@ecai16}
        \end{itemize}
    \item Perception Planning
        \begin{itemize}
            \item View planning under time constraints: \cite{kunze17ecmr}
        \end{itemize}
    \item Manipulation
        \begin{itemize}
            \item KR and Reasoning about Everyday Object Manipulation: \cite{kunze15aij} \newline Videos: 
                \begin{itemize}
                    \item grasping an egg: \url{https://www.youtube.com/watch?v=jLz87H4q3hU} 
                    \item making a pancake: \url{https://www.youtube.com/watch?v=AfVHMUK35w8}, \url{https://www.youtube.com/watch?v=YQs5gRei8k4}
                \end{itemize}
        \end{itemize}
    \item Human-Robot Interaction
        \begin{itemize}
            \item Generation of Referring Expression on Desktop Scenes: \cite{tufts17reg}
        \end{itemize}
    \item Integrated Robot Systems
        \begin{itemize}
            \item STRANDS project: \cite{strands@ram}
            \item Object search in real-world offices \cite{kunze14indirect}
            \item Object search in a multi-story building: \cite{kunze12objsearch} (Video: \url{https://www.youtube.com/watch?v=RIYRQC2iBp0})

        \end{itemize}
\end{itemize}

\begin{figure}[tb]
  \begin{center}
    \includegraphics[width=.43\columnwidth]{images/betty.jpg}
    \includegraphics[width=.55\columnwidth,clip,trim=10ex 20ex 10ex 20ex]{images/viewplanning_at_tsc.png}
  \end{center}   
  \caption{View planning in a real-world office environment (STRANDS project).}
  \label{fig:mk}
  \vspace{-3ex}
\end{figure}

\section{Conclusion}

\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document} 
